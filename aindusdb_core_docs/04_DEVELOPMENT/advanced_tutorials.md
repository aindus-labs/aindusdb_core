# üöÄ TUTORIELS AVANC√âS - AINDUSDB CORE

**Version** : 1.0.0  
**Niveau** : Expert Enterprise  
**Date** : 21 janvier 2026  

---

## üéØ **INTRODUCTION**

Tutoriels avanc√©s pour ma√Ætriser AindusDB Core en production grande √©chelle avec patterns distributed systems, chaos engineering et certifications enterprise.

### **üèÜ PR√âREQUIS**
- **Niveau Expert** : Tutoriels de base compl√©t√©s
- **Production Experience** : Environnement Docker/K8s
- **Cloud Knowledge** : AWS/Azure/GCP concepts
- **Security Background** : OWASP, ISO basics

---

## üåê **TUTORIEL 7 : DISTRIBUTED TRACING (120 minutes)**

#### **üéØ Objectif**
Impl√©menter distributed tracing avec OpenTelemetry pour monitoring microservices et debugging performance.

#### **üèóÔ∏è ARCHITECTURE TRACING**
```mermaid
graph TB
    A[Client Request] --> B[API Gateway]
    B --> C[FastAPI Service]
    C --> D[Command Bus]
    D --> E[Event Store]
    C --> F[Query Service]
    F --> G[PostgreSQL]
    C --> H[Vector Service]
    H --> I[pgvector]
    
    J[OpenTelemetry] --> K[Jaeger Collector]
    K --> L[Jaeger UI]
    
    A -.-> J
    B -.-> J
    C -.-> J
    D -.-> J
    E -.-> J
    F -.-> J
    G -.-> J
    H -.-> J
    I -.-> J
```

#### **üì¶ √âTAPE 1 : Installation OpenTelemetry**
```bash
# Installation d√©pendances tracing
pip install opentelemetry-api
pip install opentelemetry-sdk
pip install opentelemetry-instrumentation-fastapi
pip install opentelemetry-exporter-jaeger
pip install opentelemetry-exporter-otlp
pip install opentelemetry-instrumentation-asyncpg
pip install opentelemetry-instrumentation-redis
```

#### **‚öôÔ∏è √âTAPE 2 : Configuration Tracing**
```python
# app/core/tracing.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor

class DistributedTracing:
    def __init__(self, service_name: str = "aindusdb-core"):
        self.service_name = service_name
        self.tracer_provider = None
        self._setup_tracing()
    
    def _setup_tracing(self):
        # Configuration tracer provider
        self.tracer_provider = TracerProvider()
        trace.set_tracer_provider(self.tracer_provider)
        
        # Exporter Jaeger
        jaeger_exporter = JaegerExporter(
            agent_host_name="localhost",
            agent_port=6831,
            endpoint="http://localhost:14268/api/traces",
        )
        
        # Processor batch pour performance
        span_processor = BatchSpanProcessor(jaeger_exporter)
        self.tracer_provider.add_span_processor(span_processor)
        
        # Instrumentation automatique
        FastAPIInstrumentor.instrument()
        AsyncPGInstrumentor.instrument()
        RedisInstrumentor.instrument()
    
    def get_tracer(self, name: str = None):
        return trace.get_tracer(name or self.service_name)

# Initialisation tracing
tracing = DistributedTracing("aindusdb-core")
```

#### **üîç √âTAPE 3 : Tracing Personnalis√©**
```python
# app/services/vector_service.py
from app.core.tracing import tracing

class VectorService:
    def __init__(self):
        self.tracer = tracing.get_tracer("vector-service")
    
    async def search_vectors(self, query: str, filters: dict = None):
        with self.tracer.start_as_current_span("vector-search") as span:
            # Ajouter attributs span
            span.set_attribute("vector.query_length", len(query))
            span.set_attribute("vector.filters_count", len(filters or {}))
            span.set_attribute("vector.service", "aindusdb-core")
            
            try:
                # Sous-span pour embedding
                with self.tracer.start_as_current_span("generate-embedding") as embedding_span:
                    embedding_span.set_attribute("embedding.model", "sentence-transformers")
                    embedding = await self.generate_embedding(query)
                    embedding_span.set_attribute("embedding.dimensions", len(embedding))
                
                # Sous-span pour recherche PostgreSQL
                with self.tracer.start_as_current_span("postgresql-search") as db_span:
                    db_span.set_attribute("db.operation", "vector_similarity_search")
                    results = await self.search_in_database(embedding, filters)
                    db_span.set_attribute("db.results_count", len(results))
                
                # Sous-span pour post-traitement
                with self.tracer.start_as_current_span("post-processing") as processing_span:
                    processed_results = await self.process_results(results)
                    processing_span.set_attribute("processing.algorithm", "relevance-scoring")
                
                span.set_attribute("vector.final_results", len(processed_results))
                return processed_results
                
            except Exception as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise
```

#### **üìä √âTAPE 4 : Monitoring Jaeger**
```yaml
# docker-compose.tracing.yml
version: '3.8'
services:
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # HTTP collector
      - "6831:6831"    # UDP agent
    environment:
      - COLLECTOR_OTLP_ENABLED=true
  
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/tracing-prometheus.yml:/etc/prometheus/prometheus.yml
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./monitoring/grafana-tracing:/etc/grafana/provisioning
```

#### **üîç √âTAPE 5 : Analyse Performance**
```python
# app/core/performance_analyzer.py
import time
from opentelemetry import trace

class PerformanceAnalyzer:
    def __init__(self):
        self.tracer = trace.get_tracer("performance-analyzer")
    
    async def analyze_request_flow(self, request_id: str):
        with self.tracer.start_as_current_span("performance-analysis") as span:
            # R√©cup√©rer traces Jaeger
            traces = await self.get_jaeger_traces(request_id)
            
            # Analyser latences
            latency_analysis = self.analyze_latencies(traces)
            span.set_attribute("analysis.total_latency", latency_analysis['total'])
            span.set_attribute("analysis.bottleneck", latency_analysis['bottleneck'])
            
            # Analyser erreurs
            error_analysis = self.analyze_errors(traces)
            span.set_attribute("analysis.error_rate", error_analysis['rate'])
            span.set_attribute("analysis.error_types", error_analysis['types'])
            
            # Recommandations optimisation
            recommendations = self.generate_recommendations(
                latency_analysis, error_analysis
            )
            
            return {
                "request_id": request_id,
                "latency": latency_analysis,
                "errors": error_analysis,
                "recommendations": recommendations
            }
    
    def analyze_latencies(self, traces):
        spans = traces['spans']
        total_latency = 0
        service_latencies = {}
        
        for span in spans:
            service = span['service']
            duration = span['duration']
            total_latency += duration
            
            if service not in service_latencies:
                service_latencies[service] = []
            service_latencies[service].append(duration)
        
        # Identifier bottleneck
        bottleneck_service = max(
            service_latencies.keys(),
            key=lambda s: max(service_latencies[s])
        )
        
        return {
            "total": total_latency,
            "by_service": {
                s: {
                    "avg": sum(l) / len(l),
                    "max": max(l),
                    "min": min(l)
                } for s, l in service_latencies.items()
            },
            "bottleneck": bottleneck_service
        }
```

#### **üéØ R√©sultats Attendus**
- **Tracing complet** : Toutes les requ√™tes trac√©es
- **Performance monitoring** : Latences par service
- **Error tracking** : Exceptions et erreurs identifi√©es
- **Optimization insights** : Bottlenecks d√©tect√©s

---

## üå™Ô∏è **TUTORIEL 8 : CHAOS ENGINEERING (150 minutes)**

#### **üéØ Objectif**
Impl√©menter chaos engineering pour tester la r√©silience et am√©liorer la fiabilit√© en production.

#### **üèóÔ∏è ARCHITECTURE CHAOS**
```mermaid
graph TB
    A[Chaos Controller] --> B[Chaos Experiments]
    B --> C[Network Latency]
    B --> D[Service Failures]
    B --> E[Resource Exhaustion]
    B --> F[Data Corruption]
    
    G[Monitoring] --> H[Metrics Collection]
    H --> I[Alerting]
    I --> J[Auto-Recovery]
    
    K[Chaos Dashboard] --> L[Experiment Results]
    L --> M[Resilience Score]
    M --> N[Improvement Actions]
```

#### **‚ö° √âTAPE 1 : Installation Chaos Mesh**
```bash
# Installation Chaos Mesh (Kubernetes)
curl -sSL https://mirrors.chaos-mesh.org/v1.1.0/install.sh | bash

# Namespace chaos testing
kubectl create namespace chaos-testing

# D√©ployer Chaos Mesh
kubectl apply -f chaos-mesh.yaml
```

#### **üîß √âTAPE 2 : Configuration Chaos Engine**
```python
# app/core/chaos_engine.py
import asyncio
import random
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class ChaosType(Enum):
    NETWORK_LATENCY = "network_latency"
    SERVICE_FAILURE = "service_failure"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    DATA_CORRUPTION = "data_corruption"

@dataclass
class ChaosExperiment:
    name: str
    chaos_type: ChaosType
    target_service: str
    parameters: Dict[str, Any]
    duration: int
    rollback_strategy: str

class ChaosEngine:
    def __init__(self):
        self.experiments = []
        self.monitoring = MonitoringService()
        self.rollback_manager = RollbackManager()
    
    async def execute_experiment(self, experiment: ChaosExperiment):
        print(f"üå™Ô∏è Ex√©cution exp√©rience chaos: {experiment.name}")
        
        try:
            # Baseline monitoring
            baseline = await self.monitoring.capture_baseline(experiment.target_service)
            
            # Inject chaos
            await self.inject_chaos(experiment)
            
            # Monitoring pendant chaos
            chaos_metrics = await self.monitor_during_chaos(experiment)
            
            # Analyse r√©silience
            resilience_score = self.calculate_resilience(baseline, chaos_metrics)
            
            # Rollback
            await self.rollback_manager.rollback(experiment)
            
            return {
                "experiment": experiment.name,
                "resilience_score": resilience_score,
                "metrics": chaos_metrics,
                "recommendations": self.generate_recommendations(resilience_score)
            }
            
        except Exception as e:
            await self.emergency_rollback(experiment)
            raise ChaosExperimentException(f"√âchec exp√©rience: {e}")
    
    async def inject_chaos(self, experiment: ChaosExperiment):
        if experiment.chaos_type == ChaosType.NETWORK_LATENCY:
            await self.inject_network_latency(experiment)
        elif experiment.chaos_type == ChaosType.SERVICE_FAILURE:
            await self.inject_service_failure(experiment)
        elif experiment.chaos_type == ChaosType.RESOURCE_EXHAUSTION:
            await self.inject_resource_exhaustion(experiment)
        elif experiment.chaos_type == ChaosType.DATA_CORRUPTION:
            await self.inject_data_corruption(experiment)
    
    async def inject_network_latency(self, experiment: ChaosExperiment):
        # Injection latence r√©seau
        latency_ms = experiment.parameters.get('latency_ms', 1000)
        jitter = experiment.parameters.get('jitter', 100)
        
        chaos_config = {
            "apiVersion": "chaos-mesh.org/v1alpha1",
            "kind": "NetworkChaos",
            "metadata": {
                "name": f"latency-{experiment.name}",
                "namespace": "chaos-testing"
            },
            "spec": {
                "action": "delay",
                "mode": "one",
                "selector": {
                    "labelSelectors": {
                        "app": experiment.target_service
                    }
                },
                "delay": {
                    "latency": f"{latency_ms}ms",
                    "jitter": f"{jitter}ms",
                    "correlation": "0"
                },
                "duration": f"{experiment.duration}s"
            }
        }
        
        await self.k8s_client.apply(chaos_config)
        print(f"üåê Latence {latency_ms}ms inject√©e dans {experiment.target_service}")
```

#### **üß™ √âTAPE 3 : Exp√©riences Chaos**
```python
# experiments/chaos_scenarios.py

class ChaosScenarios:
    def __init__(self):
        self.engine = ChaosEngine()
    
    async def scenario_1_database_latency(self):
        """Test latence base de donn√©es"""
        experiment = ChaosExperiment(
            name="database-latency-test",
            chaos_type=ChaosType.NETWORK_LATENCY,
            target_service="postgres",
            parameters={
                "latency_ms": 2000,  # 2 secondes
                "jitter": 500
            },
            duration=300,  # 5 minutes
            rollback_strategy="immediate"
        )
        
        return await self.engine.execute_experiment(experiment)
    
    async def scenario_2_api_service_failure(self):
        """Test d√©faillance service API"""
        experiment = ChaosExperiment(
            name="api-service-failure",
            chaos_type=ChaosType.SERVICE_FAILURE,
            target_service="aindusdb-api",
            parameters={
                "failure_rate": 0.3,  # 30% √©chec
                "error_type": "http_500"
            },
            duration=600,  # 10 minutes
            rollback_strategy="gradual"
        )
        
        return await self.engine.execute_experiment(experiment)
    
    async def scenario_3_memory_exhaustion(self):
        """Test √©puisement m√©moire"""
        experiment = ChaosExperiment(
            name="memory-exhaustion",
            chaos_type=ChaosType.RESOURCE_EXHAUSTION,
            target_service="aindusdb-api",
            parameters={
                "memory_limit": "100Mi",  # Limite basse
                "stress_duration": "2m"
            },
            duration=180,  # 3 minutes
            rollback_strategy="immediate"
        )
        
        return await self.engine.execute_experiment(experiment)
    
    async def scenario_4_redis_connection_loss(self):
        """Test perte connexion Redis"""
        experiment = ChaosExperiment(
            name="redis-connection-loss",
            chaos_type=ChaosType.NETWORK_LATENCY,
            target_service="redis",
            parameters={
                "latency_ms": 10000,  # 10 secondes = timeout
                "packet_loss": 0.5   # 50% paquets perdus
            },
            duration=240,  # 4 minutes
            rollback_strategy="immediate"
        )
        
        return await self.engine.execute_experiment(experiment)
```

#### **üìä √âTAPE 4 : Monitoring Chaos**
```python
# app/core/chaos_monitoring.py
class ChaosMonitoring:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
    
    async def monitor_chaos_experiment(self, experiment: ChaosExperiment):
        monitoring_data = {
            "experiment_name": experiment.name,
            "start_time": datetime.utcnow(),
            "metrics": {},
            "alerts": [],
            "recovery_time": None
        }
        
        # Monitoring continu pendant exp√©rience
        start_time = time.time()
        
        while time.time() - start_time < experiment.duration:
            # Collecter m√©triques
            current_metrics = await self.collect_metrics(experiment.target_service)
            monitoring_data["metrics"][time.time()] = current_metrics
            
            # V√©rifier alertes
            alerts = await self.check_alerts(current_metrics, experiment.baseline)
            if alerts:
                monitoring_data["alerts"].extend(alerts)
                await self.alert_manager.send_alert(alerts)
            
            # V√©rifier r√©cup√©ration automatique
            if await self.check_auto_recovery(current_metrics):
                monitoring_data["recovery_time"] = time.time() - start_time
                break
            
            await asyncio.sleep(10)  # Check every 10 seconds
        
        return monitoring_data
    
    async def collect_metrics(self, service: str):
        return {
            "cpu_usage": await self.get_cpu_usage(service),
            "memory_usage": await self.get_memory_usage(service),
            "response_time": await self.get_response_time(service),
            "error_rate": await self.get_error_rate(service),
            "throughput": await self.get_throughput(service),
            "connection_pool": await self.get_connection_pool_status(service)
        }
```

#### **üìà √âTAPE 5 : Analyse R√©silience**
```python
# app/core/resilience_analyzer.py
class ResilienceAnalyzer:
    def __init__(self):
        self.baseline_thresholds = {
            "response_time_p95": 1000,  # ms
            "error_rate": 0.01,         # 1%
            "cpu_usage": 0.8,           # 80%
            "memory_usage": 0.85,        # 85%
            "throughput_degradation": 0.2 # 20%
        }
    
    def calculate_resilience_score(self, baseline: dict, chaos_metrics: dict):
        scores = {}
        
        # Score r√©ponse temps
        baseline_p95 = baseline["response_time"]["p95"]
        chaos_p95 = chaos_metrics["response_time"]["p95"]
        response_time_score = min(1.0, baseline_p95 / chaos_p95)
        scores["response_time"] = response_time_score
        
        # Score taux d'erreur
        baseline_error_rate = baseline["error_rate"]
        chaos_error_rate = chaos_metrics["error_rate"]
        error_score = max(0.0, 1.0 - (chaos_error_rate - baseline_error_rate) / 0.5)
        scores["error_rate"] = error_score
        
        # Score disponibilit√©
        availability_score = 1.0 - chaos_metrics.get("downtime_percentage", 0) / 100
        scores["availability"] = availability_score
        
        # Score r√©cup√©ration
        recovery_time = chaos_metrics.get("recovery_time", float('inf'))
        recovery_score = max(0.0, 1.0 - recovery_time / 300)  # 5 minutes max
        scores["recovery"] = recovery_score
        
        # Score global
        overall_score = sum(scores.values()) / len(scores)
        
        return {
            "overall": overall_score,
            "breakdown": scores,
            "grade": self.get_resilience_grade(overall_score)
        }
    
    def get_resilience_grade(self, score: float):
        if score >= 0.9:
            return "A+ (Excellent)"
        elif score >= 0.8:
            return "A (Tr√®s Bon)"
        elif score >= 0.7:
            return "B (Bon)"
        elif score >= 0.6:
            return "C (Moyen)"
        else:
            return "D (√Ä Am√©liorer)"
```

#### **üéØ R√©sultats Attendus**
- **R√©silience mesur√©e** : Score A+ √† D
- **Bottlenecks identifi√©s** : Points de faiblesse
- **Auto-r√©cup√©ration** : Capacit√©s syst√®me
- **Recommandations** : Actions d'am√©lioration

---

## üè≠ **TUTORIEL 9 : PRODUCTION GRANDE √âCHELLE (180 minutes)**

#### **üéØ Objectif**
D√©ployer AindusDB Core en production grande √©chelle avec millions de vecteurs et haute disponibilit√©.

#### **üèóÔ∏è ARCHITECTURE MULTI-R√âGION**
```mermaid
graph TB
    subgraph "Region US-East"
        A1[Load Balancer] --> A2[API Cluster 3x]
        A2 --> A3[PostgreSQL Primary]
        A2 --> A4[Redis Cluster]
        A2 --> A5[pgvector Nodes 3x]
    end
    
    subgraph "Region EU-West"
        B1[Load Balancer] --> B2[API Cluster 3x]
        B2 --> B3[PostgreSQL Standby]
        B2 --> B4[Redis Cluster]
        B2 --> B5[pgvector Nodes 3x]
    end
    
    subgraph "Region APAC"
        C1[Load Balancer] --> C2[API Cluster 2x]
        C2 --> C3[PostgreSQL Standby]
        C2 --> C4[Redis Cluster]
        C2 --> C5[pgvector Nodes 2x]
    end
    
    A3 -.->|Replication| B3
    B3 -.->|Replication| C3
    
    D[Global CDN] --> A1
    D --> B1
    D --> C1
```

#### **‚òÅÔ∏è √âTAPE 1 : Configuration AWS Multi-R√©gion**
```python
# infrastructure/aws-multi-region.py
import boto3
from typing import Dict, List

class AWSMultiRegionDeployer:
    def __init__(self):
        self.regions = ['us-east-1', 'eu-west-1', 'ap-southeast-1']
        self.clients = {region: boto3.client('ec2', region_name=region) 
                       for region in self.regions}
    
    async def deploy_vpc_infrastructure(self):
        vpcs = {}
        
        for region in self.regions:
            print(f"üåê D√©ploiement VPC dans {region}")
            
            # Cr√©er VPC
            vpc_response = self.clients[region].create_vpc(
                CidrBlock='10.0.0.0/16',
                TagSpecifications=[
                    {
                        'ResourceType': 'vpc',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'aindusdb-vpc-{region}'},
                            {'Key': 'Environment', 'Value': 'production'}
                        ]
                    }
                ]
            )
            
            vpc_id = vpc_response['Vpc']['VpcId']
            
            # Cr√©er subnets
            subnets = await self.create_subnets(region, vpc_id)
            
            # Cr√©er security groups
            security_groups = await self.create_security_groups(region, vpc_id)
            
            vpcs[region] = {
                'vpc_id': vpc_id,
                'subnets': subnets,
                'security_groups': security_groups
            }
        
        return vpcs
    
    async def deploy_rds_cluster(self, vpcs: Dict):
        """D√©ployer cluster PostgreSQL multi-AZ"""
        
        # Primary region (US-East)
        primary_client = boto3.client('rds', region_name='us-east-1')
        
        db_cluster = primary_client.create_db_cluster(
            DBClusterIdentifier='aindusdb-cluster',
            Engine='aurora-postgresql',
            EngineVersion='14.6',
            MasterUsername='aindusdb_admin',
            MasterUserPassword=self.generate_secure_password(),
            AvailabilityZones=['us-east-1a', 'us-east-1b', 'us-east-1c'],
            BackupRetentionPeriod=7,
            PreferredBackupWindow='03:00-04:00',
            PreferredMaintenanceWindow='sun:04:00-sun:05:00',
            DeletionProtection=True,
            StorageEncrypted=True,
            KmsKeyId='alias/aindusdb-kms',
            EnableCloudwatchLogsExports=['postgresql'],
            DBClusterParameterGroupName='aindusdb-postgres-parameter-group',
            VpcSecurityGroupIds=[vpcs['us-east-1']['security_groups']['database']],
            DBSubnetGroupName='aindusdb-subnet-group'
        )
        
        # Add instances
        for i in range(3):
            primary_client.create_db_instance(
                DBInstanceIdentifier=f'aindusdb-instance-{i+1}',
                DBClusterIdentifier='aindusdb-cluster',
                DBInstanceClass='db.r6g.2xlarge',
                Engine='aurora-postgresql',
                PubliclyAccessible=False,
                StorageType='aurora',
                EnablePerformanceInsights=True,
                PerformanceInsightsRetentionPeriod=7,
                MonitoringInterval=60,
                MonitoringRoleArn='arn:aws:iam::account:role/rds-enhanced-monitoring'
            )
        
        # Cross-region replicas
        await self.create_cross_region_replicas(vpcs)
        
        return db_cluster
```

#### **‚ö° √âTAPE 2 : Kubernetes Scaling**
```yaml
# k8s/production-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aindusdb-api
  namespace: production
spec:
  replicas: 10  # Scaling horizontal
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 2
  selector:
    matchLabels:
      app: aindusdb-api
      version: v1.0.0
  template:
    metadata:
      labels:
        app: aindusdb-api
        version: v1.0.0
    spec:
      containers:
      - name: aindusdb
        image: aindusdb/core:1.0.0
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: aindusdb-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: aindusdb-secrets
              key: redis-url
        - name: JWT_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: aindusdb-secrets
              key: jwt-secret
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aindusdb-api-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: aindusdb-api
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
```

#### **üìä √âTAPE 3 : Monitoring Production**
```python
# monitoring/production_monitoring.py
class ProductionMonitoring:
    def __init__(self):
        self.prometheus = PrometheusClient()
        self.grafana = GrafanaClient()
        self.alertmanager = AlertManagerClient()
    
    async def setup_comprehensive_monitoring(self):
        # Configuration m√©triques custom
        metrics_config = {
            "vector_operations": {
                "type": "counter",
                "name": "vector_operations_total",
                "labels": ["operation_type", "status"]
            },
            "embedding_generation_time": {
                "type": "histogram",
                "name": "embedding_generation_seconds",
                "buckets": [0.1, 0.5, 1.0, 2.0, 5.0]
            },
            "database_connections": {
                "type": "gauge",
                "name": "database_connections_active"
            },
            "cache_hit_ratio": {
                "type": "gauge",
                "name": "cache_hit_ratio"
            }
        }
        
        # Configuration alertes
        alert_rules = [
            {
                "name": "HighErrorRate",
                "condition": "rate(http_requests_total{status=~'5..'}[5m]) > 0.01",
                "severity": "critical",
                "action": "scale_up"
            },
            {
                "name": "HighLatency",
                "condition": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2",
                "severity": "warning",
                "action": "investigate"
            },
            {
                "name": "DatabaseConnectionsHigh",
                "condition": "database_connections_active > 80",
                "severity": "warning",
                "action": "scale_db"
            }
        ]
        
        await self.setup_metrics(metrics_config)
        await self.setup_alerts(alert_rules)
        await self.setup_dashboards()
```

#### **üöÄ √âTAPE 4 : Performance Optimization**
```python
# performance/production_optimization.py
class ProductionOptimizer:
    def __init__(self):
        self.connection_pool = ConnectionPool()
        self.cache_manager = CacheManager()
        self.query_optimizer = QueryOptimizer()
    
    async def optimize_for_million_vectors(self):
        # Optimisations base de donn√©es
        optimizations = {
            "postgresql": {
                "shared_buffers": "8GB",
                "effective_cache_size": "24GB",
                "work_mem": "256MB",
                "maintenance_work_mem": "2GB",
                "max_connections": 200,
                "pgvector_ivfflat_probe": 10
            },
            "redis": {
                "maxmemory": "16GB",
                "maxmemory-policy": "allkeys-lru",
                "tcp-keepalive": 300,
                "timeout": 0
            },
            "application": {
                "uvicorn_workers": 8,
                "worker_connections": 2000,
                "backlog": 2048,
                "limit_concurrency": 1000,
                "limit_max_requests": 10000,
                "limit_max_requests_jitter": 1000
            }
        }
        
        # Indexation vectorielle optimis√©e
        await self.create_optimized_vector_indexes()
        
        # Partitionnement donn√©es
        await self.setup_data_partitioning()
        
        # Cache multi-niveau
        await self.setup_multi_level_cache()
        
        return optimizations
    
    async def create_optimized_vector_indexes(self):
        """Cr√©er indexes vectoriels optimis√©s pour millions de vecteurs"""
        
        # Index IVFFlat avec param√®tres optimis√©s
        index_sql = """
        CREATE INDEX CONCURRENTLY vectors_embedding_ivfflat 
        ON vectors USING ivfflat (embedding vector_cosine_ops) 
        WITH (lists = 1000);
        
        -- Index HNSW pour haute pr√©cision
        CREATE INDEX CONCURRENTLY vectors_embedding_hnsw 
        ON vectors USING hnsw (embedding vector_cosine_ops) 
        WITH (m = 16, ef_construction = 64);
        """
        
        await self.database.execute(index_sql)
        
        # Analyser pour optimiser
        await self.database.execute("ANALYZE vectors;")
    
    async def setup_data_partitioning(self):
        """Partitionnement par date pour performance"""
        
        partition_sql = """
        CREATE TABLE vectors_partitioned (
            LIKE vectors INCLUDING ALL
        ) PARTITION BY RANGE (created_at);
        
        -- Cr√©er partitions mensuelles
        CREATE TABLE vectors_2026_01 PARTITION OF vectors_partitioned
        FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
        
        CREATE TABLE vectors_2026_02 PARTITION OF vectors_partitioned
        FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');
        """
        
        await self.database.execute(partition_sql)
```

#### **üìà √âTAPE 5 : Scaling Tests**
```python
# performance/scaling_tests.py
class ScalingTestSuite:
    def __init__(self):
        self.load_generator = LoadGenerator()
        self.metrics_collector = MetricsCollector()
    
    async def run_million_vector_test(self):
        """Test avec 1 million de vecteurs"""
        
        # Phase 1: Insertion massive
        print("üìä Phase 1: Insertion 1M vecteurs...")
        insertion_results = await self.test_massive_insertion(1000000)
        
        # Phase 2: Recherche concurrente
        print("üîç Phase 2: Recherche concurrente...")
        search_results = await self.test_concurrent_search(1000)
        
        # Phase 3: Scaling test
        print("‚ö° Phase 3: Test scaling...")
        scaling_results = await self.test_horizontal_scaling()
        
        return {
            "insertion": insertion_results,
            "search": search_results,
            "scaling": scaling_results,
            "overall_score": self.calculate_overall_score(
                insertion_results, search_results, scaling_results
            )
        }
    
    async def test_massive_insertion(self, vector_count: int):
        """Test insertion massive avec monitoring"""
        
        batch_size = 1000
        batches = vector_count // batch_size
        
        start_time = time.time()
        successful_inserts = 0
        failed_inserts = 0
        
        for batch_num in range(batches):
            batch_start = time.time()
            
            # G√©n√©rer batch
            vectors = self.generate_test_vectors(batch_size)
            
            # Ins√©rer avec monitoring
            try:
                result = await self.insert_vectors_batch(vectors)
                successful_inserts += result['inserted_count']
                
                # Monitoring performance
                batch_time = time.time() - batch_start
                throughput = batch_size / batch_time
                
                print(f"Batch {batch_num+1}/{batches}: {throughput:.0f} vectors/sec")
                
                # M√©triques temps r√©el
                await self.metrics_collector.record_metric(
                    "batch_insertion_throughput", throughput
                )
                
            except Exception as e:
                failed_inserts += batch_size
                print(f"‚ùå √âchec batch {batch_num+1}: {e}")
        
        total_time = time.time() - start_time
        overall_throughput = successful_inserts / total_time
        
        return {
            "total_vectors": successful_inserts,
            "failed_vectors": failed_inserts,
            "total_time": total_time,
            "throughput_vectors_per_sec": overall_throughput,
            "success_rate": successful_inserts / vector_count
        }
```

#### **üéØ R√©sultats Attendus**
- **1M+ vecteurs** : Insertion et recherche performante
- **99.99% uptime** : Haute disponibilit√©
- **Sub-second latency** : R√©ponses < 1 seconde
- **Auto-scaling** : 5-50 pods automatiquement

---

## üí∞ **TUTORIEL 10 : OPTIMISATION CO√õTS (90 minutes)**

#### **üéØ Objectif**
Optimiser les co√ªts cloud tout en maintenant performance et disponibilit√©.

#### **üí° STRAT√âGIES OPTIMISATION**
1. **Right-sizing** : Dimensionnement optimal ressources
2. **Spot instances** : Utilisation instances spot
3. **Auto-scaling** : Scaling dynamique
4. **Storage optimization** : Compression et archivage
5. **Network optimization** : CDN et caching

#### **‚öôÔ∏è √âTAPE 1 : Cost Monitoring**
```python
# cost/cost_monitor.py
class CostMonitor:
    def __init__(self):
        self.aws_ce = boto3.client('ce', region_name='us-east-1')
        self.azure_cost = AzureCostManagementClient()
        self.gcp_billing = GCPBillingClient()
    
    async def get_cost_analysis(self, start_date: str, end_date: str):
        """Analyse co√ªts d√©taill√©e"""
        
        # AWS Cost Explorer
        aws_costs = await self.aws_ce.get_cost_and_usage(
            TimePeriod={
                'Start': start_date,
                'End': end_date
            },
            Granularity='DAILY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {'Type': 'DIMENSION', 'Key': 'SERVICE'},
                {'Type': 'DIMENSION', 'Key': 'INSTANCE_TYPE'}
            ]
        )
        
        # Analyse par service
        cost_breakdown = self.analyze_costs_by_service(aws_costs)
        
        # Identification optimisations
        optimizations = await self.identify_cost_optimizations(cost_breakdown)
        
        return {
            "total_cost": cost_breakdown['total'],
            "by_service": cost_breakdown['services'],
            "by_instance": cost_breakdown['instances'],
            "optimizations": optimizations,
            "potential_savings": optimizations['total_savings']
        }
    
    async def identify_cost_optimizations(self, cost_breakdown):
        """Identifier opportunit√©s d'optimisation"""
        
        optimizations = {
            "compute": [],
            "storage": [],
            "network": [],
            "database": [],
            "total_savings": 0
        }
        
        # Optimisations compute
        if cost_breakdown['instances']['ec2']['large'] > 1000:
            # Suggestion spot instances
            spot_savings = cost_breakdown['instances']['ec2']['large'] * 0.7
            optimizations["compute"].append({
                "type": "spot_instances",
                "description": "Utiliser spot instances pour 70% des workloads",
                "savings": spot_savings,
                "risk": "medium"
            })
            optimizations["total_savings"] += spot_savings
        
        # Optimisations storage
        if cost_breakdown['services']['s3'] > 500:
            # Suggestion lifecycle policies
            storage_savings = cost_breakdown['services']['s3'] * 0.3
            optimizations["storage"].append({
                "type": "lifecycle_policies",
                "description": "Archiver vieux donn√©es vers Glacier",
                "savings": storage_savings,
                "risk": "low"
            })
            optimizations["total_savings"] += storage_savings
        
        return optimizations
```

#### **üöÄ √âTAPE 2 : Spot Instances Strategy**
```python
# infrastructure/spot_instances.py
class SpotInstanceManager:
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.autoscaling = boto3.client('autoscaling')
    
    async def setup_spot_fleet(self):
        """Configurer fleet spot instances"""
        
        spot_fleet_request = {
            "SpotFleetRequestConfig": {
                "IamFleetRole": "arn:aws:iam::account:role/aws-ec2-spot-fleet-tagging-role",
                "TargetCapacity": 20,
                "SpotPrice": "0.05",
                "AllocationStrategy": "diversified",
                "LaunchSpecifications": [
                    {
                        "ImageId": "ami-12345678",
                        "InstanceType": "c5.large",
                        "SubnetId": "subnet-12345",
                        "WeightedCapacity": 1,
                        "TagSpecifications": [
                            {
                                "ResourceType": "instance",
                                "Tags": [
                                    {"Key": "Name", "Value": "aindusdb-spot"},
                                    {"Key": "Environment", "Value": "production"}
                                ]
                            }
                        ]
                    },
                    {
                        "ImageId": "ami-12345678",
                        "InstanceType": "c5.xlarge",
                        "SubnetId": "subnet-12345",
                        "WeightedCapacity": 2,
                        "TagSpecifications": [
                            {
                                "ResourceType": "instance", 
                                "Tags": [
                                    {"Key": "Name", "Value": "aindusdb-spot"},
                                    {"Key": "Environment", "Value": "production"}
                                ]
                            }
                        ]
                    }
                ],
                "ExcessCapacityTerminationPolicy": "default"
            }
        }
        
        response = self.ec2.request_spot_fleet(**spot_fleet_request)
        return response['SpotFleetRequestId']
    
    async def handle_spot_interruptions(self):
        """G√©rer interruptions spot instances"""
        
        # Monitoring spot events
        spot_events = await self.monitor_spot_events()
        
        for event in spot_events:
            if event['instance_id'] and event['status'] == 'interruption-warning':
                # D√©marrer nouvelle instance avant interruption
                await self.launch_replacement_instance()
                
                # Graceful shutdown
                await self.graceful_shutdown_instance(event['instance_id'])
```

#### **üíæ √âTAPE 3 : Storage Optimization**
```python
# storage/storage_optimizer.py
class StorageOptimizer:
    def __init__(self):
        self.s3 = boto3.client('s3')
        self.glacier = boto3.client('glacier')
    
    async def setup_lifecycle_policies(self):
        """Configurer politiques de cycle de vie"""
        
        lifecycle_config = {
            'Rules': [
                {
                    'ID': 'AindusDBVectorData',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'vectors/'
                    },
                    'Transitions': [
                        {
                            'Days': 30,
                            'StorageClass': 'STANDARD_IA'
                        },
                        {
                            'Days': 90,
                            'StorageClass': 'GLACIER'
                        },
                        {
                            'Days': 365,
                            'StorageClass': 'DEEP_ARCHIVE'
                        }
                    ],
                    'Expiration': {
                        'Days': 2555  # 7 years
                    }
                },
                {
                    'ID': 'AindusDBLogs',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'logs/'
                    },
                    'Transitions': [
                        {
                            'Days': 7,
                            'StorageClass': 'STANDARD_IA'
                        },
                        {
                            'Days': 30,
                            'StorageClass': 'GLACIER'
                        }
                    ],
                    'Expiration': {
                        'Days': 365
                    }
                }
            ]
        }
        
        self.s3.put_bucket_lifecycle_configuration(
            Bucket='aindusdb-data',
            LifecycleConfiguration=lifecycle_config
        )
    
    async def compress_vector_data(self):
        """Compresser donn√©es vectorielles"""
        
        compression_results = {
            "original_size": 0,
            "compressed_size": 0,
            "savings": 0
        }
        
        # Parcourir objets S3
        for obj in self.s3.list_objects_v2(Bucket='aindusdb-data', Prefix='vectors/')['Contents']:
            key = obj['Key']
            
            # T√©l√©charger et compresser
            original_data = self.s3.get_object(Bucket='aindusdb-data', Key=key)['Body'].read()
            compressed_data = gzip.compress(original_data)
            
            # Uploader version compress√©e
            compressed_key = key.replace('.json', '.json.gz')
            self.s3.put_object(
                Bucket='aindusdb-data',
                Key=compressed_key,
                Body=compressed_data,
                ContentEncoding='gzip'
            )
            
            # Supprimer original
            self.s3.delete_object(Bucket='aindusdb-data', Key=key)
            
            # Mettre √† jour statistiques
            compression_results['original_size'] += len(original_data)
            compression_results['compressed_size'] += len(compressed_data)
        
        compression_results['savings'] = (
            compression_results['original_size'] - compression_results['compressed_size']
        ) / compression_results['original_size'] * 100
        
        return compression_results
```

#### **üéØ R√©sultats Attendus**
- **30-70% √©conomies** : Spot instances, storage optimization
- **Performance maintenue** : Auto-scaling intelligent
- **Monitoring co√ªts** : Suivi en temps r√©el
- **Alertes budget** : Pr√©vention surco√ªts

---

## üèÜ **CERTIFICATIONS PR√âPARATION**

### **üîí OWASP CERTIFICATION**
```python
# certification/owasp_prep.py
class OWASPCertificationPrep:
    def __init__(self):
        self.security_checker = SecurityChecker()
        self.compliance_monitor = ComplianceMonitor()
    
    async def prepare_owasp_certification(self):
        """Pr√©parer certification OWASP"""
        
        # Checklist OWASP Top 10 2021
        owasp_checklist = {
            "A01_Broken_Access_Control": await self.test_access_control(),
            "A02_Cryptographic_Failures": await self.test_cryptography(),
            "A03_Injection": await self.test_injection_protection(),
            "A04_Insecure_Design": await self.test_secure_design(),
            "A05_Security_Misconfiguration": await self.test_security_config(),
            "A06_Vulnerable_Components": await self.test_component_vulnerabilities(),
            "A07_Authentication_Failures": await self.test_authentication(),
            "A08_Software_Integrity": await self.test_software_integrity(),
            "A09_Logging_Monitoring": await self.test_logging_monitoring(),
            "A10_Server_Side_Request_Forgery": await self.test_ssrf_protection()
        }
        
        # Calculer score OWASP
        owasp_score = sum(owasp_checklist.values()) / len(owasp_checklist)
        
        return {
            "owasp_score": owasp_score,
            "checklist": owasp_checklist,
            "certification_ready": owasp_score >= 0.85
        }
```

### **üèÖ ISO 27001 CERTIFICATION**
```python
# certification/iso27001_prep.py
class ISO27001Prep:
    def __init__(self):
        self.isms = InformationSecurityManagementSystem()
        self.audit_trail = AuditTrailSystem()
    
    async def prepare_iso27001(self):
        """Pr√©parer certification ISO 27001"""
        
        # Annex A controls
        iso_controls = {
            "A.5_Organisation": await self.test_organization_security(),
            "A.6_People": await self.test_people_security(),
            "A.7_Physical": await self.test_physical_security(),
            "A.8_Technology": await self.test_technology_security(),
            "A.9_Communications": await self.test_communications_security(),
            "A.10_System_Acquisition": await self.test_system_acquisition(),
            "A.11_Supplier_Relationships": await self.test_supplier_security(),
            "A.12_Incident_Management": await self.test_incident_management(),
            "A.13_Business_Continuity": await self.test_business_continuity(),
            "A.14_Compliance": await self.test_compliance()
        }
        
        # Documentation ISMS
        isms_documentation = await self.generate_isms_documentation()
        
        return {
            "iso_controls": iso_controls,
            "documentation": isms_documentation,
            "certification_ready": all(control['compliant'] for control in iso_controls.values())
        }
```

### **‚òÅÔ∏è CLOUD PROVIDER CERTIFICATIONS**
```python
# certification/cloud_certifications.py
class CloudCertificationPrep:
    def __init__(self):
        self.aws_prep = AWSCertificationPrep()
        self.azure_prep = AzureCertificationPrep()
        self.gcp_prep = GCPCertificationPrep()
    
    async def prepare_aws_solutions_architect(self):
        """Pr√©parer AWS Solutions Architect Professional"""
        
        aws_domains = {
            "Design_Resilient_Architectures": await self.test_resilient_design(),
            "Design_Performant_Architectures": await self.test_performance_design(),
            "Design_Secure_Applications": await self.test_secure_design(),
            "Design_Cost_Optimized_Architectures": await self.test_cost_optimization(),
            "Design_Operational_Excellence": await self.test_operational_excellence()
        }
        
        return aws_domains
    
    async def prepare_azure_architect(self):
        """Pr√©parer Azure Solutions Architect Expert"""
        
        azure_domains = {
            "Design_Monitoring": await self.test_azure_monitoring(),
            "Design_Identity_Security": await self.test_azure_security(),
            "Design_Data_Storage": await self.test_azure_storage(),
            "Design_Business_Continuity": await self.test_azure_continuity(),
            "Design_Infrastructure": await self.test_azure_infrastructure()
        }
        
        return azure_domains
```

---

## üéØ **CONCLUSION**

### **üèÜ COMP√âTENCES AVANC√âS MA√éTRIS√âES**

#### **üåê Distributed Tracing**
- ‚úÖ OpenTelemetry implementation
- ‚úÖ Jaeger monitoring complet
- ‚úÖ Performance analysis
- ‚úÖ Bottleneck identification

#### **üå™Ô∏è Chaos Engineering**
- ‚úÖ Chaos Mesh deployment
- ‚úÖ R√©silience testing
- ‚úÖ Auto-recovery validation
- ‚úÖ Resilience scoring A+

#### **üè≠ Production Grande √âchelle**
- ‚úÖ Multi-region deployment
- ‚úÖ 1M+ vectors performance
- ‚úÖ 99.99% availability
- ‚úÖ Auto-scaling 5-50 pods

#### **üí∞ Cost Optimization**
- ‚úÖ 30-70% savings
- ‚úÖ Spot instances strategy
- ‚úÖ Storage optimization
- ‚úÖ Real-time cost monitoring

#### **üèÜ Certifications**
- ‚úÖ OWASP 8.5/10 ready
- ‚úÖ ISO 27001 compliance
- ‚úÖ Cloud architect prep
- ‚úÖ Enterprise audit ready

### **üöÄ PROCHAINES √âTAPES**

1. **Production Deployment** : Mise en production r√©elle
2. **Customer Onboarding** : Support clients enterprise
3. **Continuous Improvement** : Optimisations continues
4. **Innovation R&D** : Nouvelles fonctionnalit√©s

**Vous √™tes maintenant un expert AindusDB Core niveau production mondiale !** üéâ

---

*Tutoriels Avanc√©s - 21 janvier 2026*  
*Formation Expert Enterprise*
