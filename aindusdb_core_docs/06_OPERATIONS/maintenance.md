# Maintenance et Mises √† Jour - AindusDB Core

**Version:** 1.0  
**Date:** 21/01/2026  
**Auteur:** √âquipe AindusDB  
**Statut:** En r√©daction  

---

## üîß Vue d'ensemble

Ce guide d√©crit les proc√©dures de maintenance r√©guli√®res et les strat√©gies de mise √† jour pour assurer la stabilit√© et la performance de AindusDB Core.

---

## üìÖ Calendrier de Maintenance

### Maintenance Quotidienne

```yaml
daily_maintenance:
  time: "02:00 UTC"
  duration: "15 minutes"
  tasks:
    - name: "Log Rotation"
      script: "scripts/rotate_logs.sh"
      impact: "none"
    
    - name: "Cache Cleanup"
      script: "scripts/cleanup_cache.py"
      impact: "none"
    
    - name: "Health Checks"
      script: "scripts/health_check.py"
      impact: "none"
    
    - name: "Metrics Collection"
      script: "scripts/collect_metrics.py"
      impact: "none"
```

### Maintenance Hebdomadaire

```yaml
weekly_maintenance:
  day: "Sunday"
  time: "03:00 UTC"
  duration: "1 hour"
  tasks:
    - name: "Database Optimization"
      script: "scripts/optimize_db.py"
      impact: "read_only"
    
    - name: "Security Scan"
      script: "scripts/security_scan.sh"
      impact: "none"
    
    - name: "Backup Verification"
      script: "scripts/verify_backups.py"
      impact: "none"
    
    - name: "Performance Analysis"
      script: "scripts/performance_analysis.py"
      impact: "none"
```

### Maintenance Mensuelle

```yaml
monthly_maintenance:
  day: "First Sunday"
  time: "02:00 UTC"
  duration: "4 hours"
  tasks:
    - name: "System Updates"
      script: "scripts/system_updates.sh"
      impact: "downtime"
    
    - name: "Index Rebuild"
      script: "scripts/rebuild_indexes.py"
      impact: "read_only"
    
    - name: "Full Security Audit"
      script: "scripts/security_audit.py"
      impact: "none"
    
    - name: "Capacity Planning"
      script: "scripts/capacity_planning.py"
      impact: "none"
```

---

## üîÑ Proc√©dures de Maintenance

### 1. Rotation des Logs

```bash
#!/bin/bash
# scripts/rotate_logs.sh

set -e

LOG_DIR="/var/log/aindusdb"
RETENTION_DAYS=30
BACKUP_DIR="/backup/logs"

echo "Starting log rotation - $(date)"

# Cr√©er le r√©pertoire de backup s'il n'existe pas
mkdir -p $BACKUP_DIR

# Rotation des logs d'application
for log_file in $LOG_DIR/*.log; do
    if [ -f "$log_file" ]; then
        timestamp=$(date +%Y%m%d-%H%M%S)
        filename=$(basename $log_file .log)
        
        # Compresser et d√©placer
        gzip -c $log_file > $BACKUP_DIR/${filename}-${timestamp}.log.gz
        
        # Vider le fichier actuel
        > $log_file
        
        echo "Rotated $log_file"
    fi
done

# Nettoyer les anciens logs
find $BACKUP_DIR -name "*.log.gz" -mtime +$RETENTION_DAYS -delete

# Rotation des logs syst√®me
logrotate -f /etc/logrotate.d/aindusdb

# Envoyer une notification de succ√®s
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-type: application/json' \
  --data "{\"text\":\"‚úÖ Log rotation completed successfully\"}"

echo "Log rotation completed - $(date)"
```

### 2. Nettoyage du Cache

```python
# scripts/cleanup_cache.py
import asyncio
import redis
from datetime import datetime, timedelta
from app.core.config import settings
from app.database import get_db

async def cleanup_cache():
    """Nettoie le cache Redis"""
    r = redis.Redis(
        host=settings.redis_host,
        port=settings.redis_port,
        password=settings.redis_password,
        decode_responses=True
    )
    
    # 1. Nettoyer les cl√©s expir√©es
    expired_keys = 0
    for key in r.scan_iter(match="*"):
        ttl = r.ttl(key)
        if ttl == -1:  # Pas d'expiration
            # V√©rifier si la cl√© est vieille de plus de 24h
            # Note: n√©cessite un tracking s√©par√©
            pass
        elif ttl == -2:  # Cl√© expir√©e
            r.delete(key)
            expired_keys += 1
    
    # 2. Nettoyer les sessions expir√©es
    session_keys = r.keys("session:*")
    for key in session_keys:
        session_data = r.hgetall(key)
        if session_data:
            last_access = int(session_data.get('last_access', 0))
            if datetime.now().timestamp() - last_access > 86400:  # 24h
                r.delete(key)
    
    # 3. Nettoyer les r√©sultats de calculs VERITAS anciens
    veritas_keys = r.keys("veritas:*")
    cutoff = datetime.now() - timedelta(days=7)
    
    for key in veritas_keys:
        created_at = r.hget(key, 'created_at')
        if created_at:
            if datetime.fromisoformat(created_at) < cutoff:
                r.delete(key)
    
    # 4. Optimiser la m√©moire Redis
    r.memory_purge()
    
    # Statistiques
    info = r.info()
    print(f"Cache cleanup completed:")
    print(f"  - Expired keys removed: {expired_keys}")
    print(f"  - Memory used: {info['used_memory_human']}")
    print(f"  - Memory peak: {info['used_memory_peak_human']}")
    
    # Notification
    await send_notification("Cache cleanup completed", {
        "expired_keys": expired_keys,
        "memory_usage": info['used_memory_human']
    })

if __name__ == "__main__":
    asyncio.run(cleanup_cache())
```

### 3. Optimisation de la Base de Donn√©es

```python
# scripts/optimize_db.py
import asyncio
import asyncpg
from datetime import datetime
from app.core.config import settings

async def optimize_database():
    """Optimise la base de donn√©es PostgreSQL"""
    
    conn = await asyncpg.connect(
        host=settings.db_host,
        port=settings.db_port,
        user=settings.db_user,
        password=settings.db_password,
        database=settings.db_name
    )
    
    try:
        print("Starting database optimization...")
        
        # 1. Analyser les statistiques
        print("Updating table statistics...")
        await conn.execute("ANALYZE")
        
        # 2. Nettoyer les tables mortes
        print("Vacuuming tables...")
        tables = await conn.fetch("""
            SELECT schemaname, tablename
            FROM pg_tables
            WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
        """)
        
        for table in tables:
            schema = table['schemaname']
            name = table['tablename']
            print(f"  Vacuuming {schema}.{name}")
            await conn.execute(f"VACUUM ANALYZE {schema}.{name}")
        
        # 3. R√©organiser les indexes fragment√©s
        print("Checking fragmented indexes...")
        fragmented_indexes = await conn.fetch("""
            SELECT schemaname, tablename, indexname, 
                   pg_size_pretty(pg_relation_size(indexrelid)) as size,
                   round(100 * (1 - avg_leaf_density)) as fragmentation
            FROM pg_stat_user_indexes
            WHERE round(100 * (1 - avg_leaf_density)) > 30
            ORDER BY fragmentation DESC
        """)
        
        for index in fragmented_indexes:
            schema = index['schemaname']
            table = index['tablename']
            idx_name = index['indexname']
            
            print(f"  Rebuilding index {schema}.{idx_name}")
            await conn.execute(f"REINDEX INDEX CONCURRENTLY {schema}.{idx_name}")
        
        # 4. Nettoyer les anciennes donn√©es
        print("Cleaning old data...")
        
        # Logs plus anciens que 30 jours
        await conn.execute("""
            DELETE FROM logs 
            WHERE created_at < NOW() - INTERVAL '30 days'
        """)
        
        # Sessions expir√©es
        await conn.execute("""
            DELETE FROM user_sessions 
            WHERE expires_at < NOW()
        """)
        
        # 5. Mettre √† jour les statistiques du planificateur
        print("Updating planner statistics...")
        await conn.execute("UPDATE STATISTICS")
        
        # 6. V√©rifier l'espace lib√©r√©
        space_freed = await conn.fetch_one("""
            SELECT pg_size_pretty(pg_database_size(current_database()) 
                   - (SELECT pg_database_size(current_database()) 
                      FROM pg_database WHERE datname = current_database()
                      OFFSET 1 ROW)) as freed_space
        """)
        
        print(f"Database optimization completed!")
        print(f"  - Space freed: {space_freed['freed_space']}")
        print(f"  - Fragmented indexes rebuilt: {len(fragmented_indexes)}")
        
        # Notification
        await send_notification("Database optimization completed", {
            "space_freed": space_freed['freed_space'],
            "indexes_rebuilt": len(fragmented_indexes)
        })
        
    finally:
        await conn.close()

if __name__ == "__main__":
    asyncio.run(optimize_database())
```

### 4. V√©rification des Backups

```python
# scripts/verify_backups.py
import asyncio
import boto3
import gzip
import tempfile
from datetime import datetime, timedelta
from app.core.config import settings

class BackupVerifier:
    def __init__(self):
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=settings.aws_access_key_id,
            aws_secret_access_key=settings.aws_secret_access_key,
            region_name=settings.aws_region
        )
        self.backup_bucket = settings.aws_s3_bucket
    
    async def verify_recent_backups(self):
        """V√©rifie les backups des 24 derni√®res heures"""
        yesterday = datetime.utcnow() - timedelta(days=1)
        prefix = f"backups/{yesterday.strftime('%Y/%m/%d')}/"
        
        # Lister les backups
        backups = self.s3_client.list_objects_v2(
            Bucket=self.backup_bucket,
            Prefix=prefix
        )
        
        if not backups.get('Contents'):
            raise Exception("No backups found for yesterday!")
        
        verification_results = []
        
        for backup in backups['Contents']:
            result = await self.verify_backup(backup['Key'])
            verification_results.append(result)
        
        # G√©n√©rer le rapport
        await self.generate_verification_report(verification_results)
        
        return verification_results
    
    async def verify_backup(self, backup_key: str):
        """V√©rifie l'int√©grit√© d'un backup"""
        print(f"Verifying backup: {backup_key}")
        
        # 1. T√©l√©charger le backup
        with tempfile.NamedTemporaryFile() as tmp_file:
            self.s3_client.download_file(
                self.backup_bucket,
                backup_key,
                tmp_file.name
            )
            
            # 2. V√©rifier l'int√©grit√©
            if backup_key.endswith('.gz'):
                with gzip.open(tmp_file.name, 'rb') as f:
                    # V√©rifier que le fichier est un gzip valide
                    try:
                        f.read(1024)  # Lire les premiers KB
                        is_valid = True
                    except:
                        is_valid = False
            else:
                # Pour les fichiers non compress√©s
                is_valid = True
            
            # 3. V√©rifier la taille
            size = tmp_file.tell()
            
            return {
                "backup_key": backup_key,
                "size": size,
                "is_valid": is_valid,
                "verified_at": datetime.utcnow()
            }
    
    async def test_backup_restore(self):
        """Test la restauration d'un backup"""
        # R√©cup√©rer le backup le plus r√©cent
        latest_backup = self.get_latest_backup()
        
        # Restaurer dans un environnement de test
        test_db_name = f"test_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            # Cr√©er la base de test
            await self.create_test_database(test_db_name)
            
            # Restaurer le backup
            await self.restore_backup(latest_backup['Key'], test_db_name)
            
            # V√©rifier l'int√©grit√© des donn√©es
            integrity_check = await self.verify_restored_data(test_db_name)
            
            # Nettoyer
            await self.drop_test_database(test_db_name)
            
            return {
                "backup": latest_backup['Key'],
                "restore_successful": True,
                "integrity_check": integrity_check
            }
            
        except Exception as e:
            await self.drop_test_database(test_db_name)
            return {
                "backup": latest_backup['Key'],
                "restore_successful": False,
                "error": str(e)
            }

if __name__ == "__main__":
    verifier = BackupVerifier()
    
    # V√©rifier les backups r√©cents
    results = asyncio.run(verifier.verify_recent_backups())
    
    # Tester une restauration
    test_result = asyncio.run(verifier.test_backup_restore())
    
    print(f"Backup verification completed!")
    print(f"  - Backups verified: {len(results)}")
    print(f"  - Restore test: {'‚úÖ' if test_result['restore_successful'] else '‚ùå'}")
```

---

## üöÄ Proc√©dures de Mise √† Jour

### Strat√©gie de Rolling Update

```yaml
# k8s/rolling-update.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: aindusdb-api
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app: aindusdb-api
  template:
    metadata:
      labels:
        app: aindusdb-api
    spec:
      containers:
      - name: api
        image: aindusdb/core:1.2.3
        ports:
        - containerPort: 8000
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
  minReadySeconds: 10
```

### Script de Mise √† Jour Automatis√©e

```bash
#!/bin/bash
# scripts/update_aindusdb.sh

set -e

NEW_VERSION=$1
if [ -z "$NEW_VERSION" ]; then
    echo "Usage: $0 <new_version>"
    exit 1
fi

echo "üöÄ Starting AindusDB update to version $NEW_VERSION"
echo "Timestamp: $(date)"

# 1. Pr√©-v√©rifications
echo "üìã Running pre-update checks..."
./scripts/pre_update_checks.sh

# 2. Cr√©er un backup
echo "üíæ Creating backup..."
./scripts/create_backup.sh pre-update-$NEW_VERSION

# 3. Mettre √† jour la configuration
echo "‚öôÔ∏è Updating configuration..."
kubectl set image deployment/aindusdb-api api=aindusdb/core:$NEW_VERSION
kubectl set image deployment/aindusdb-worker worker=aindusdb/core:$NEW_VERSION

# 4. Attendre le rolling update
echo "‚è≥ Waiting for rolling update to complete..."
kubectl rollout status deployment/aindusdb-api --timeout=600s
kubectl rollout status deployment/aindusdb-worker --timeout=600s

# 5. Ex√©cuter les migrations de base de donn√©es
echo "üóÑÔ∏è Running database migrations..."
kubectl exec deployment/aindusdb-api -- alembic upgrade head

# 6. Post-v√©rifications
echo "‚úÖ Running post-update checks..."
./scripts/post_update_checks.sh

# 7. Nettoyer les anciennes versions
echo "üßπ Cleaning up old versions..."
kubectl delete pods -l app=aindusdb-api --field-selector=status.phase=Succeeded
kubectl delete pods -l app=aindusdb-worker --field-selector=status.phase=Succeeded

# 8. Notification de succ√®s
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-type: application/json' \
  --data "{\"text\":\"üéâ AindusDB successfully updated to version $NEW_VERSION\"}"

echo "‚úÖ Update completed successfully!"
```

### Tests de Mise √† Jour

```python
# scripts/test_update.py
import asyncio
import aiohttp
import pytest
from typing import List, Dict

class UpdateTester:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.test_results = []
    
    async def run_all_tests(self) -> Dict:
        """Ex√©cute tous les tests post-mise √† jour"""
        tests = [
            self.test_api_endpoints,
            self.test_database_connectivity,
            self.test_vector_operations,
            self.test_veritas_calculations,
            self.test_authentication,
            self.test_performance,
            self.test_monitoring
        ]
        
        for test in tests:
            try:
                result = await test()
                self.test_results.append(result)
            except Exception as e:
                self.test_results.append({
                    "test": test.__name__,
                    "status": "failed",
                    "error": str(e)
                })
        
        return self.generate_report()
    
    async def test_api_endpoints(self):
        """Test les endpoints de l'API"""
        endpoints = [
            "/health",
            "/health/ready",
            "/health/live",
            "/v1/users/me",
            "/v1/vector/indexes"
        ]
        
        results = []
        async with aiohttp.ClientSession() as session:
            for endpoint in endpoints:
                async with session.get(f"{self.base_url}{endpoint}") as response:
                    results.append({
                        "endpoint": endpoint,
                        "status": response.status,
                        "success": response.status < 400
                    })
        
        return {
            "test": "API Endpoints",
            "status": "passed" if all(r["success"] for r in results) else "failed",
            "details": results
        }
    
    async def test_vector_operations(self):
        """Test les op√©rations vectorielles"""
        async with aiohttp.ClientSession() as session:
            # Cr√©er un index
            create_response = await session.post(
                f"{self.base_url}/v1/vectors/indexes",
                json={
                    "name": "test_update",
                    "dimension": 1536,
                    "metric": "cosine"
                }
            )
            
            if create_response.status != 201:
                return {
                    "test": "Vector Operations",
                    "status": "failed",
                    "error": "Failed to create index"
                }
            
            index_data = await create_response.json()
            index_id = index_data["id"]
            
            # Ins√©rer des vecteurs
            insert_response = await session.post(
                f"{self.base_url}/v1/vectors/indexes/{index_id}/upsert",
                json={
                    "vectors": [{
                        "id": "test_1",
                        "values": [0.1] * 1536,
                        "metadata": {"test": True}
                    }]
                }
            )
            
            # Rechercher
            search_response = await session.post(
                f"{self.base_url}/v1/vectors/indexes/{index_id}/query",
                json={
                    "vector": [0.1] * 1536,
                    "top_k": 1
                }
            )
            
            # Nettoyer
            await session.delete(
                f"{self.base_url}/v1/vectors/indexes/{index_id}"
            )
            
            return {
                "test": "Vector Operations",
                "status": "passed" if search_response.status == 200 else "failed",
                "operations": {
                    "create": create_response.status,
                    "insert": insert_response.status,
                    "search": search_response.status
                }
            }
    
    async def test_veritas_calculations(self):
        """Test les calculs VERITAS"""
        test_cases = [
            {"query": "sqrt(16)", "expected": "4.0"},
            {"query": "2 + 3", "expected": "5.0"},
            {"query": "sin(pi/2)", "expected": "1.0"}
        ]
        
        results = []
        async with aiohttp.ClientSession() as session:
            for case in test_cases:
                response = await session.post(
                    f"{self.base_url}/v1/veritas/calculate",
                    json={
                        "query": case["query"],
                        "verification_level": "standard"
                    }
                )
                
                if response.status == 200:
                    data = await response.json()
                    results.append({
                        "query": case["query"],
                        "expected": case["expected"],
                        "actual": data["answer"],
                        "success": data["answer"] == case["expected"]
                    })
                else:
                    results.append({
                        "query": case["query"],
                        "success": False,
                        "error": response.status
                    })
        
        return {
            "test": "VERITAS Calculations",
            "status": "passed" if all(r["success"] for r in results) else "failed",
            "results": results
        }
    
    def generate_report(self) -> Dict:
        """G√©n√®re un rapport de test"""
        passed = len([r for r in self.test_results if r["status"] == "passed"])
        failed = len([r for r in self.test_results if r["status"] == "failed"])
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "summary": {
                "total": len(self.test_results),
                "passed": passed,
                "failed": failed
            },
            "tests": self.test_results,
            "overall_status": "passed" if failed == 0 else "failed"
        }

if __name__ == "__main__":
    tester = UpdateTester("http://api.aindusdb.com")
    report = asyncio.run(tester.run_all_tests())
    
    print(f"Update test results:")
    print(f"  Passed: {report['summary']['passed']}")
    print(f"  Failed: {report['summary']['failed']}")
    print(f"  Overall: {report['overall_status']}")
```

---

## üìä Gestion des Versions

### Strat√©gie de Versionnement

```yaml
version_strategy:
  format: "MAJOR.MINOR.PATCH"
  
  major:
    description: "Changes incompatible with previous versions"
    examples:
      - "Database schema changes"
      - "API breaking changes"
      - "Major architecture updates"
    release_frequency: "Annually"
  
  minor:
    description: "New features in a backwards compatible manner"
    examples:
      - "New API endpoints"
      - "New vector operations"
      - "Performance improvements"
    release_frequency: "Quarterly"
  
  patch:
    description: "Backwards compatible bug fixes"
    examples:
      - "Bug fixes"
      - "Security patches"
      - "Small improvements"
    release_frequency: "As needed"
```

### Pipeline de Release

```yaml
# .github/workflows/release.yml
name: Release Pipeline

on:
  push:
    tags:
      - 'v*'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Tests
        run: pytest tests/ -v
  
  security_scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Security Scan
        run: |
          pip install bandit
          bandit -r app/ -f json -o bandit-report.json
  
  build_and_publish:
    needs: [test, security_scan]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: aindusdb/core
      
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            aindusdb/core:latest
            aindusdb/core:${{ github.ref_name }}
          labels: ${{ steps.meta.outputs.labels }}
  
  deploy_staging:
    needs: build_and_publish
    runs-on: ubuntu-latest
    environment: staging
    steps:
      - name: Deploy to Staging
        run: |
          kubectl set image deployment/aindusdb-staging \
            api=aindusdb/core:${{ github.ref_name }}
          kubectl rollout status deployment/aindusdb-staging
  
  production_approval:
    needs: deploy_staging
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Wait for Approval
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: team-leads
          minimum-approvals: 2
  
  deploy_production:
    needs: production_approval
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Production
        run: |
          ./scripts/update_aindusdb.sh ${{ github.ref_name }}
      
      - name: Create Release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ github.ref_name }}
          release_name: Release ${{ github.ref_name }}
          draft: false
          prerelease: false
```

---

## üîÑ Gestion du Changement

### Processus de Change Advisory Board (CAB)

```python
# scripts/change_management.py
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional
import smtplib
from email.mime.text import MIMEText

@dataclass
class ChangeRequest:
    title: str
    description: str
    risk_level: str  # low, medium, high, critical
    impact: str
    rollback_plan: str
    test_plan: str
    submitter: str
    approvers: List[str]
    scheduled_date: datetime
    estimated_duration: int  # minutes

class ChangeManager:
    def __init__(self):
        self.pending_changes = []
        self.approved_changes = []
        self.change_history = []
    
    def submit_change(self, change: ChangeRequest):
        """Soumet une demande de changement"""
        change.id = self.generate_change_id()
        change.status = "pending"
        change.submitted_at = datetime.utcnow()
        
        self.pending_changes.append(change)
        
        # Notifier les approbateurs
        self.notify_approvers(change)
        
        return change.id
    
    def approve_change(self, change_id: str, approver: str, comments: str = ""):
        """Approuve un changement"""
        change = self.find_change(change_id)
        if not change:
            raise ValueError("Change not found")
        
        if approver not in change.approvers:
            raise ValueError("Not an approved approver")
        
        # Ajouter l'approbation
        if not hasattr(change, 'approvals'):
            change.approvals = []
        
        change.approvals.append({
            "approver": approver,
            "timestamp": datetime.utcnow(),
            "comments": comments
        })
        
        # V√©rifier si toutes les approbations sont obtenues
        if len(change.approvals) == len(change.approvers):
            change.status = "approved"
            self.pending_changes.remove(change)
            self.approved_changes.append(change)
            
            # Notifier l'approbation
            self.notify_approval(change)
    
    def schedule_change(self, change_id: str, scheduled_date: datetime):
        """Planifie un changement approuv√©"""
        change = self.find_change(change_id)
        if change.status != "approved":
            raise ValueError("Change must be approved first")
        
        change.scheduled_date = scheduled_date
        change.status = "scheduled"
        
        # Cr√©er une entr√©e dans le calendrier
        self.create_calendar_event(change)
    
    def execute_change(self, change_id: str):
        """Ex√©cute un changement planifi√©"""
        change = self.find_change(change_id)
        if change.status != "scheduled":
            raise ValueError("Change must be scheduled first")
        
        try:
            # Notifier le d√©but
            self.notify_change_start(change)
            
            # Ex√©cuter le changement
            result = self.run_change_script(change)
            
            # V√©rifier le succ√®s
            if result["success"]:
                change.status = "completed"
                change.completed_at = datetime.utcnow()
                
                # Notifier le succ√®s
                self.notify_change_success(change)
            else:
                change.status = "failed"
                change.error = result["error"]
                
                # Initier le rollback
                self.initiate_rollback(change)
            
            # Ajouter √† l'historique
            self.change_history.append(change)
            
        except Exception as e:
            change.status = "failed"
            change.error = str(e)
            self.initiate_rollback(change)
    
    def generate_change_calendar(self):
        """G√©n√®re un calendrier des changements √† venir"""
        from icalendar import Calendar, Event
        
        cal = Calendar()
        cal.add('prodid', '-//AindusDB Change Calendar//')
        cal.add('version', '2.0')
        
        for change in self.approved_changes:
            if change.status == "scheduled":
                event = Event()
                event.add('summary', change.title)
                event.add('description', change.description)
                event.add('dtstart', change.scheduled_date)
                event.add('dtend', change.scheduled_date + timedelta(minutes=change.estimated_duration))
                event.add('location', 'Production Environment')
                event.add('status', 'CONFIRMED')
                
                cal.add_component(event)
        
        return cal.to_ical()
```

---

## üìã Checklists de Maintenance

### Checklist Pre-Maintenance

```markdown
## Pre-Maintenance Checklist

### Planning
- [ ] Maintenance window approved and communicated
- [ ] All stakeholders notified
- [ ] Back-up procedures verified
- [ ] Rollback plan documented
- [ ] Test environment prepared

### System Preparation
- [ ] Current system state documented
- [ ] Performance baselines recorded
- [ ] Critical processes identified
- [ ] Emergency contacts verified
- [ ] Monitoring tools ready

### Data Protection
- [ ] Full backup completed
- [ ] Backup integrity verified
- [ ] Backup stored off-site
- [ ] Recovery procedures tested
- [ ] Data encryption verified

### Communication
- [ ] Status page updated
- [ ] Customer notifications sent
- [ ] Internal team notified
- [ ] Incident response team on standby
- [ ] Communication channels tested
```

### Checklist Post-Maintenance

```markdown
## Post-Maintenance Checklist

### Verification
- [ ] All services started successfully
- [ ] Health checks passing
- [ ] Performance within baselines
- [ ] Data integrity verified
- [ ] Security controls active

### Testing
- [ ] Critical functionality tested
- [ ] User acceptance verified
- [ ] Integration points tested
- [ ] Error handling verified
- [ ] Load testing completed

### Documentation
- [ ] Changes documented
- [ ] Configuration updated
- [ ] Runbooks updated
- [ ] Knowledge base updated
- [ ] Lessons learned recorded

### Cleanup
- [ ] Temporary files removed
- [ ] Old versions cleaned up
- [ ] Logs archived
- [ ] Monitoring alerts reset
- [ ] System optimized
```

---

## üéØ Bonnes Pratiques

### 1. Planification
- Maintenance pendant les heures creuses
- Communication proactive avec les utilisateurs
- Tests complets en environnement de staging
- Plans de rollback d√©taill√©s

### 2. Ex√©cution
- Suivi strict des proc√©dures
- Monitoring continu pendant les op√©rations
- Documentation en temps r√©el
- Points de contr√¥le d√©finis

### 3. V√©rification
- Tests automatis√©s post-maintenance
- Validation par les utilisateurs
- Comparaison avec les baselines
- Surveillance accrue apr√®s maintenance

### 4. Am√©lioration
- Post-mortems pour chaque incident
- Mise √† jour des proc√©dures
- Formation de l'√©quipe
- Automatisation croissante

---

**Document maintenu par l'√©quipe AindusDB Core**  
**Derni√®re mise √† jour:** 21/01/2026
